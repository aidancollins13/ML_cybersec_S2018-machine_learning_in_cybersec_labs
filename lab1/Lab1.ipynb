{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.datasets as skd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model, naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to load the data this way because the skd.load_files didn't work for some reason\n",
    "bare_train_data = []\n",
    "bare_train_target = []\n",
    "bare_test_data = []\n",
    "bare_test_target = []\n",
    "path = \"lingspam_public/bare/\"\n",
    "for n in range(1,9):\n",
    "    for filename in os.listdir(path + \"part{}\".format(n)):\n",
    "        f = open(path + \"part{}/\".format(n) +filename, \"r\")\n",
    "        bare_train_data.append(f.read())\n",
    "        bare_train_target.append((filename[:3] == 'spm'))\n",
    "        \n",
    "for filename in os.listdir(path+\"part10\"):\n",
    "    f = open(path+\"part10/\"+filename, \"r\")\n",
    "    bare_test_data.append(f.read())\n",
    "    bare_test_target.append((filename[:3] == 'spm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemm_train_data = []\n",
    "lemm_train_target = []\n",
    "lemm_test_data = []\n",
    "lemm_test_target = []\n",
    "path = \"lingspam_public/lemm/\"\n",
    "for n in range(1,9):\n",
    "    for filename in os.listdir(path + \"part{}\".format(n)):\n",
    "        f = open(path + \"part{}/\".format(n) +filename, \"r\")\n",
    "        lemm_train_data.append(f.read())\n",
    "        lemm_train_target.append((filename[:3] == 'spm'))\n",
    "        \n",
    "for filename in os.listdir(path+\"part10\"):\n",
    "    f = open(path+\"part10/\"+filename, \"r\")\n",
    "    lemm_test_data.append(f.read())\n",
    "    lemm_test_target.append((filename[:3] == 'spm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemm_stop_train_data = []\n",
    "lemm_stop_train_target = []\n",
    "lemm_stop_test_data = []\n",
    "lemm_stop_test_target = []\n",
    "path = \"lingspam_public/lemm_stop/\"\n",
    "for n in range(1,9):\n",
    "    for filename in os.listdir(path + \"part{}\".format(n)):\n",
    "        f = open(path + \"part{}/\".format(n) +filename, \"r\")\n",
    "        lemm_stop_train_data.append(f.read())\n",
    "        lemm_stop_train_target.append((filename[:3] == 'spm'))\n",
    "        \n",
    "for filename in os.listdir(path+\"part10\"):\n",
    "    f = open(path+\"part10/\"+filename, \"r\")\n",
    "    lemm_stop_test_data.append(f.read())\n",
    "    lemm_stop_test_target.append((filename[:3] == 'spm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 53456)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_bare = count_vect.fit_transform(bare_train_data)\n",
    "X_test_bare = count_vect.transform(bare_test_data)\n",
    "X_test_bare.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm = count_vect.fit_transform(lemm_train_data)\n",
    "X_test_lemm = count_vect.transform(lemm_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm_stop = count_vect.fit_transform(lemm_stop_train_data)\n",
    "X_test_lemm_stop = count_vect.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# this function gets the Informatino Gain on each word and returns the words sorted on IG\n",
    "def get_words_IG(data, y, words):\n",
    "    IG = np.zeros(data.shape[1])\n",
    "    for j in range(0,data.shape[1]):\n",
    "        IG[j] = sklearn.metrics.mutual_info_score(data[:,j].toarray()[:,0], y)\n",
    "    return [x for _,x in sorted(zip(-IG,words))], sorted(-IG)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I get a sorted list of words\n",
    "words,IG = get_words_IG(X_test_lemm_stop, lemm_stop_test_target, count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1998',\n",
       " 'language',\n",
       " 'university',\n",
       " 'linguistic',\n",
       " 'papers',\n",
       " 'conference',\n",
       " 'remove',\n",
       " 'click',\n",
       " 'free',\n",
       " 'research']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I then create all the datasets i need for testing using sklearn\n",
    "count_vect_tf_10 = CountVectorizer(vocabulary=words[:10])\n",
    "X_train_tf_10 = count_vect_tf_10.fit_transform(lemm_stop_train_data)\n",
    "X_test_tf_10 = count_vect_tf_10.transform(lemm_stop_test_data)\n",
    "count_vect_tf_10.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_tf_100 = CountVectorizer(vocabulary=words[:100])\n",
    "X_train_tf_100 = count_vect_tf_100.fit_transform(lemm_stop_train_data)\n",
    "X_test_tf_100 = count_vect_tf_100.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_tf_1000 = CountVectorizer(vocabulary=words[:1000])\n",
    "X_train_tf_1000 = count_vect_tf_1000.fit_transform(lemm_stop_train_data)\n",
    "X_test_tf_1000 = count_vect_tf_1000.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1998',\n",
       " 'language',\n",
       " 'university',\n",
       " 'linguistic',\n",
       " 'papers',\n",
       " 'conference',\n",
       " 'remove',\n",
       " 'click',\n",
       " 'free',\n",
       " 'research']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_bin_10 = CountVectorizer(vocabulary=words[:10], binary=True)\n",
    "X_train_bin_10 = count_vect_bin_10.fit_transform(lemm_stop_train_data)\n",
    "X_test_bin_10 = count_vect_bin_10.transform(lemm_stop_test_data)\n",
    "count_vect_bin_10.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_bin_100 = CountVectorizer(vocabulary=words[:100], binary=True)\n",
    "X_train_bin_100 = count_vect_bin_100.fit_transform(lemm_stop_train_data)\n",
    "X_test_bin_100 = count_vect_bin_100.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_bin_1000 = CountVectorizer(vocabulary=words[:1000], binary=True)\n",
    "X_train_bin_1000 = count_vect_bin_1000.fit_transform(lemm_stop_train_data)\n",
    "X_test_bin_1000 = count_vect_bin_1000.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern 10 acc: 0.9862542955326461\tprecision: 0.9411764705882353\trecall: 0.9795918367346939\n",
      "bern 100 acc: 0.9965635738831615\tprecision: 1.0\r",
      "ecall: 0.9795918367346939\n",
      "bern 1000 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "bern = naive_bayes.BernoulliNB()\n",
    "bern.fit(X_train_bin_10, lemm_stop_train_target)\n",
    "bern_y_hat = bern.predict(X_test_bin_10)\n",
    "bern_acc = bern.score(X_test_bin_10, lemm_stop_test_target)\n",
    "bern_prec = sklearn.metrics.precision_score(lemm_stop_test_target, bern_y_hat)\n",
    "bern_rec = sklearn.metrics.recall_score(lemm_stop_test_target, bern_y_hat)\n",
    "print(\"bern 10 acc: {}\\tprecision: {}\\trecall: {}\".format(bern_acc, bern_prec, bern_rec))\n",
    "\n",
    "bern.fit(X_train_bin_100, lemm_stop_train_target)\n",
    "bern_y_hat = bern.predict(X_test_bin_100)\n",
    "bern_acc = bern.score(X_test_bin_100, lemm_stop_test_target)\n",
    "bern_prec = sklearn.metrics.precision_score(lemm_stop_test_target, bern_y_hat)\n",
    "bern_rec = sklearn.metrics.recall_score(lemm_stop_test_target, bern_y_hat)\n",
    "print(\"bern 100 acc: {}\\tprecision: {}\\recall: {}\".format(bern_acc, bern_prec, bern_rec))\n",
    "\n",
    "bern.fit(X_train_bin_1000, lemm_stop_train_target)\n",
    "bern_y_hat = bern.predict(X_test_bin_1000)\n",
    "bern_acc = bern.score(X_test_bin_1000, lemm_stop_test_target)\n",
    "bern_prec = sklearn.metrics.precision_score(lemm_stop_test_target, bern_y_hat)\n",
    "bern_rec = sklearn.metrics.recall_score(lemm_stop_test_target, bern_y_hat)\n",
    "print(\"bern 1000 acc: {}\\tprecision: {}\\trecall: {}\".format(bern_acc, bern_prec, bern_rec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial binary  10 acc: 0.9484536082474226\tprecision: 0.9473684210526315\trecall: 0.7346938775510204\n",
      "multinomial binary  100 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n",
      "multinomial binary  1000 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "nm_bin = naive_bayes.MultinomialNB()\n",
    "nm_bin.fit(X_train_bin_10, lemm_stop_train_target)\n",
    "nm_bin_y_hat = nm_bin.predict(X_test_bin_10)\n",
    "nm_bin_acc = nm_bin.score(X_test_bin_10, lemm_stop_test_target)\n",
    "nm_bin_prec = sklearn.metrics.precision_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "nm_bin_rec = sklearn.metrics.recall_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "print(\"multinomial binary  10 acc: {}\\tprecision: {}\\trecall: {}\".format(nm_bin_acc, nm_bin_prec, nm_bin_rec))\n",
    "\n",
    "nm_bin = naive_bayes.MultinomialNB()\n",
    "nm_bin.fit(X_train_bin_100, lemm_stop_train_target)\n",
    "nm_bin_y_hat = nm_bin.predict(X_test_bin_100)\n",
    "nm_bin_acc = nm_bin.score(X_test_bin_100, lemm_stop_test_target)\n",
    "nm_bin_prec = sklearn.metrics.precision_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "nm_bin_rec = sklearn.metrics.recall_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "print(\"multinomial binary  100 acc: {}\\tprecision: {}\\trecall: {}\".format(nm_bin_acc, nm_bin_prec, nm_bin_rec))\n",
    "\n",
    "\n",
    "nm_bin = naive_bayes.MultinomialNB()\n",
    "nm_bin.fit(X_train_bin_1000, lemm_stop_train_target)\n",
    "nm_bin_y_hat = nm_bin.predict(X_test_bin_1000)\n",
    "nm_bin_acc = nm_bin.score(X_test_bin_1000, lemm_stop_test_target)\n",
    "nm_bin_prec = sklearn.metrics.precision_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "nm_bin_rec = sklearn.metrics.recall_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "print(\"multinomial binary  1000 acc: {}\\tprecision: {}\\trecall: {}\".format(nm_bin_acc, nm_bin_prec, nm_bin_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial TF  10 acc: 0.9484536082474226\tprecision: 0.9473684210526315\trecall: 0.7346938775510204\n",
      "multinomial TF  100 acc: 0.9965635738831615\tprecision: 1.0\trecall: 0.9795918367346939\n",
      "multinomial TF  1000 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "mn_tf = naive_bayes.MultinomialNB()\n",
    "mn_tf.fit(X_train_tf_10, lemm_stop_train_target)\n",
    "mn_tf_y_hat = mn_tf.predict(X_test_tf_10)\n",
    "mn_tf_acc = mn_tf.score(X_test_tf_10, lemm_stop_test_target)\n",
    "mn_tf_prec = sklearn.metrics.precision_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "mn_tf_rec = sklearn.metrics.recall_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "print(\"multinomial TF  10 acc: {}\\tprecision: {}\\trecall: {}\".format(mn_tf_acc, mn_tf_prec, mn_tf_rec))\n",
    "\n",
    "mn_tf = naive_bayes.MultinomialNB()\n",
    "mn_tf.fit(X_train_tf_100, lemm_stop_train_target)\n",
    "mn_tf_y_hat = mn_tf.predict(X_test_tf_100)\n",
    "mn_tf_acc = mn_tf.score(X_test_tf_100, lemm_stop_test_target)\n",
    "mn_tf_prec = sklearn.metrics.precision_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "mn_tf_rec = sklearn.metrics.recall_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "print(\"multinomial TF  100 acc: {}\\tprecision: {}\\trecall: {}\".format(mn_tf_acc, mn_tf_prec, mn_tf_rec))\n",
    "\n",
    "mn_tf = naive_bayes.MultinomialNB()\n",
    "mn_tf.fit(X_train_tf_1000, lemm_stop_train_target)\n",
    "mn_tf_y_hat = mn_tf.predict(X_test_tf_1000)\n",
    "mn_tf_acc = mn_tf.score(X_test_tf_1000, lemm_stop_test_target)\n",
    "mn_tf_prec = sklearn.metrics.precision_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "mn_tf_rec = sklearn.metrics.recall_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "print(\"multinomial TF  1000 acc: {}\\tprecision: {}\\trecall: {}\".format(mn_tf_acc, mn_tf_prec, mn_tf_rec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I then begin to create my own SVM to evaluate the data\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "SV = svm.SVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  :  0.9568221070811744\n",
      "20  :  0.9585492227979274\n",
      "30  :  0.9602763385146805\n",
      "40  :  0.9654576856649395\n",
      "50  :  0.9620034542314335\n",
      "60  :  0.9654576856649395\n",
      "70  :  0.9620034542314335\n",
      "80  :  0.9568221070811744\n",
      "90  :  0.9533678756476683\n",
      "100  :  0.9533678756476683\n",
      "110  :  0.9499136442141624\n",
      "120  :  0.9516407599309153\n",
      "130  :  0.9516407599309153\n",
      "140  :  0.9516407599309153\n",
      "150  :  0.9516407599309153\n",
      "160  :  0.9481865284974094\n",
      "170  :  0.9533678756476683\n",
      "180  :  0.9550949913644214\n",
      "190  :  0.9516407599309153\n",
      "200  :  0.9516407599309153\n",
      "210  :  0.9516407599309153\n",
      "220  :  0.9516407599309153\n",
      "230  :  0.9533678756476683\n",
      "240  :  0.9499136442141624\n",
      "250  :  0.9499136442141624\n",
      "260  :  0.9533678756476683\n",
      "270  :  0.9550949913644214\n",
      "280  :  0.9568221070811744\n",
      "290  :  0.9533678756476683\n",
      "300  :  0.9499136442141624\n",
      "310  :  0.9516407599309153\n",
      "320  :  0.9516407599309153\n",
      "330  :  0.9499136442141624\n",
      "340  :  0.9499136442141624\n",
      "350  :  0.9533678756476683\n",
      "360  :  0.9533678756476683\n",
      "370  :  0.9533678756476683\n",
      "380  :  0.9533678756476683\n",
      "390  :  0.9516407599309153\n",
      "400  :  0.9499136442141624\n",
      "410  :  0.9464594127806563\n",
      "420  :  0.9447322970639033\n",
      "430  :  0.9447322970639033\n",
      "440  :  0.9481865284974094\n",
      "450  :  0.9481865284974094\n",
      "460  :  0.9481865284974094\n",
      "470  :  0.9481865284974094\n",
      "480  :  0.9481865284974094\n",
      "490  :  0.9481865284974094\n",
      "500  :  0.9499136442141624\n",
      "510  :  0.9516407599309153\n",
      "520  :  0.9516407599309153\n",
      "530  :  0.9516407599309153\n",
      "540  :  0.9481865284974094\n",
      "550  :  0.9447322970639033\n",
      "560  :  0.9464594127806563\n",
      "570  :  0.9464594127806563\n",
      "580  :  0.9447322970639033\n",
      "590  :  0.9464594127806563\n",
      "600  :  0.9447322970639033\n",
      "610  :  0.9447322970639033\n",
      "620  :  0.9447322970639033\n",
      "630  :  0.9464594127806563\n",
      "640  :  0.9464594127806563\n",
      "650  :  0.9447322970639033\n",
      "660  :  0.9447322970639033\n",
      "670  :  0.9447322970639033\n",
      "680  :  0.9464594127806563\n",
      "690  :  0.9464594127806563\n",
      "700  :  0.9464594127806563\n",
      "710  :  0.9464594127806563\n",
      "720  :  0.9447322970639033\n",
      "730  :  0.9447322970639033\n",
      "740  :  0.9447322970639033\n",
      "750  :  0.9447322970639033\n",
      "760  :  0.9447322970639033\n",
      "770  :  0.9447322970639033\n",
      "780  :  0.9447322970639033\n",
      "790  :  0.9447322970639033\n",
      "800  :  0.9447322970639033\n",
      "810  :  0.9430051813471503\n",
      "820  :  0.9447322970639033\n",
      "830  :  0.9447322970639033\n",
      "840  :  0.9447322970639033\n",
      "850  :  0.9447322970639033\n",
      "860  :  0.9447322970639033\n",
      "870  :  0.9447322970639033\n",
      "880  :  0.9447322970639033\n",
      "890  :  0.9447322970639033\n",
      "900  :  0.9430051813471503\n",
      "910  :  0.9430051813471503\n",
      "920  :  0.9412780656303973\n",
      "930  :  0.9412780656303973\n",
      "940  :  0.9412780656303973\n",
      "950  :  0.9412780656303973\n",
      "960  :  0.9412780656303973\n",
      "970  :  0.9412780656303973\n",
      "980  :  0.9412780656303973\n",
      "990  :  0.9412780656303973\n"
     ]
    }
   ],
   "source": [
    "# I use cross validation to pick the best vocabulary size\n",
    "for i in range(10,1000,10):\n",
    "    cv = CountVectorizer(vocabulary=words[:i])\n",
    "    X_train_svm = cv.fit_transform(lemm_stop_train_data)\n",
    "    l = X_train_svm.shape[0]\n",
    "    trainx = X_train_svm[:(l//4)*3,:]\n",
    "    valx = X_train_svm[(l//4)*3:,:]\n",
    "    SV.fit(trainx, lemm_stop_train_target[:(l//4)*3])\n",
    "    print(i, \" : \", SV.score(valx, lemm_stop_train_target[(l//4)*3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly  :  0.8963730569948186\n",
      "linear  :  0.9861830742659758\n",
      "rbf  :  0.9654576856649395\n",
      "sigmoid  :  0.9360967184801382\n"
     ]
    }
   ],
   "source": [
    "# The best vocab size i got was 60\n",
    "# I then use cross validatoin to get the best kernel, linear was the best\n",
    "cv = CountVectorizer(vocabulary=words[:60])\n",
    "X_train_svm = cv.fit_transform(lemm_stop_train_data)\n",
    "l = X_train_svm.shape[0]\n",
    "trainx = X_train_svm[:(l//4)*3,:]\n",
    "valx = X_train_svm[(l//4)*3:,:]\n",
    "kernels = ['poly', 'linear', 'rbf', 'sigmoid']\n",
    "for k in kernels:\n",
    "    SV = svm.SVC(kernel=k)\n",
    "    SV.fit(trainx, lemm_stop_train_target[:(l//4)*3])\n",
    "    print(k, \" : \", SV.score(valx, lemm_stop_train_target[(l//4)*3:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979381443298969\n"
     ]
    }
   ],
   "source": [
    "# I then finally test it on the test data\n",
    "SV = svm.SVC(kernel='linear')\n",
    "SV.fit(X_train_svm, lemm_stop_train_target)\n",
    "X_test_svm = cv.transform(lemm_stop_test_data)\n",
    "acc = SV.score(X_test_svm, lemm_stop_test_target)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL\n",
    "#loading in the data\n",
    "eval_data = []\n",
    "eval_target = []\n",
    "path = './eval/all/'\n",
    "for filename in os.listdir(path):\n",
    "    f = open(path + filename, \"r\")\n",
    "    eval_data.append(f.read())\n",
    "    eval_target.append((filename[:3] == 'spm'))\n",
    "\n",
    "# getting the X and y\n",
    "X_eval = cv.transform(eval_data)\n",
    "eval_y_hat = SV.predict(X_eval)\n",
    "\n",
    "# Scoring the data\n",
    "acc = SV.score(X_eval, eval_target)\n",
    "prec = sklearn.metrics.precision_score(eval_target, eval_y_hat)\n",
    "rec = sklearn.metrics.recall_score(eval_target, eval_y_hat)\n",
    "\n",
    "results = open('eval/results.txt', 'w+')\n",
    "for y in eval_y_hat:\n",
    "    results.write('{}\\n'.format(int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_bin_10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5dff7e966517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extra credit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNB10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mNB10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bin_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemm_stop_train_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspam_emails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bin_10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m  \u001b[0mlemm_stop_train_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam_emails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_bin_10' is not defined"
     ]
    }
   ],
   "source": [
    "# Extra credit\n",
    "NB10 = naive_bayes.MultinomialNB()\n",
    "NB10.fit(X_train_bin_10, lemm_stop_train_target)\n",
    "spam_emails = [x for i,x in enumerate(X_train_bin_10) if  lemm_stop_train_target[i]== True]\n",
    "len(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 1 0 0 1 0]]\t to [[1 0 0 0 0 1 0 0 1 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 1 0 0 0 0 0 0 0]]\t to [[1 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 1]]\t to [[1 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 1 0 0 1 0]]\t to [[1 0 0 0 0 1 0 0 1 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 1 0 0 0 0 0 0 1 1]]\t to [[1 1 0 0 0 0 0 0 1 1]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 1 0 0 1 1]]\t to [[1 0 0 0 0 1 0 0 1 1]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 1 0 0 0 0]]\t to [[1 0 0 0 0 1 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 1 0 0 1 0]]\t to [[1 0 0 0 0 1 0 0 1 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 1 0 0 1 0]]\t to [[1 0 0 0 0 1 0 0 1 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 1 0 0 0 0 0 0 0]]\t to [[1 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 1 0 0 0 0 0 0 0]]\t to [[1 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 1 0 0 0 0 0 0 0]]\t to [[1 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 1]]\t to [[1 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 1 0 0 0 0 0 1 0]]\t to [[1 0 1 0 0 0 0 0 1 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 1 0 0 0 0 0 0 0 0]]\t to [[1 1 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "changed 1 words to classify email as legit\n",
      "from: [[0 0 0 0 0 0 0 0 0 0]]\t to [[1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_words(email, classifier):\n",
    "    for i in range(1,10-np.sum(email)):\n",
    "        for j in range(0,i):\n",
    "            for w in range(0,len(email)):\n",
    "                if(email[0,w] == 0):\n",
    "                    email[0,w] = 1\n",
    "                    if(not classifier.predict(email)):\n",
    "                        return email, i\n",
    "                    \n",
    "    return None, 0\n",
    "\n",
    "for email in spam_emails:\n",
    "    new_email, words_added = add_words(email[0].todense(), NB10)\n",
    "    if(words_added):\n",
    "        print(\"changed {} words to classify email as legit\".format(words_added))\n",
    "        print(\"from: {}\\t to {}\\n\\n\".format(email[0].todense(),new_email))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
