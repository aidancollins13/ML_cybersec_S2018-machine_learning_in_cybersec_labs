{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.datasets as skd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model, naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how i loaded the data before I saw your code using skd\n",
    "\n",
    "bare_train_data = []\n",
    "bare_train_target = []\n",
    "bare_test_data = []\n",
    "bare_test_target = []\n",
    "path = \"lingspam_public/bare/\"\n",
    "for n in range(1,9):\n",
    "    for filename in os.listdir(path + \"part{}\".format(n)):\n",
    "        f = open(path + \"part{}/\".format(n) +filename, \"r\")\n",
    "        bare_train_data.append(f.read())\n",
    "        bare_train_target.append((filename[:3] == 'spm'))\n",
    "        \n",
    "for filename in os.listdir(path+\"part10\"):\n",
    "    f = open(path+\"part10/\"+filename, \"r\")\n",
    "    bare_test_data.append(f.read())\n",
    "    bare_test_target.append((filename[:3] == 'spm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemm_train_data = []\n",
    "lemm_train_target = []\n",
    "lemm_test_data = []\n",
    "lemm_test_target = []\n",
    "path = \"lingspam_public/lemm/\"\n",
    "for n in range(1,9):\n",
    "    for filename in os.listdir(path + \"part{}\".format(n)):\n",
    "        f = open(path + \"part{}/\".format(n) +filename, \"r\")\n",
    "        lemm_train_data.append(f.read())\n",
    "        lemm_train_target.append((filename[:3] == 'spm'))\n",
    "        \n",
    "for filename in os.listdir(path+\"part10\"):\n",
    "    f = open(path+\"part10/\"+filename, \"r\")\n",
    "    lemm_test_data.append(f.read())\n",
    "    lemm_test_target.append((filename[:3] == 'spm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemm_stop_train_data = []\n",
    "lemm_stop_train_target = []\n",
    "lemm_stop_test_data = []\n",
    "lemm_stop_test_target = []\n",
    "path = \"lingspam_public/lemm_stop/\"\n",
    "for n in range(1,9):\n",
    "    for filename in os.listdir(path + \"part{}\".format(n)):\n",
    "        f = open(path + \"part{}/\".format(n) +filename, \"r\")\n",
    "        lemm_stop_train_data.append(f.read())\n",
    "        lemm_stop_train_target.append((filename[:3] == 'spm'))\n",
    "        \n",
    "for filename in os.listdir(path+\"part10\"):\n",
    "    f = open(path+\"part10/\"+filename, \"r\")\n",
    "    lemm_stop_test_data.append(f.read())\n",
    "    lemm_stop_test_target.append((filename[:3] == 'spm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 53456)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_bare = count_vect.fit_transform(bare_train_data)\n",
    "X_test_bare = count_vect.transform(bare_test_data)\n",
    "X_test_bare.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm = count_vect.fit_transform(lemm_train_data)\n",
    "X_test_lemm = count_vect.transform(lemm_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm_stop = count_vect.fit_transform(lemm_stop_train_data)\n",
    "X_test_lemm_stop = count_vect.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def get_words_IG(data, y, words):\n",
    "    IG = np.zeros(data.shape[1])\n",
    "    for j in range(0,data.shape[1]):\n",
    "        IG[j] = sklearn.metrics.mutual_info_score(data[:,j].toarray()[:,0], y)\n",
    "    return [x for _,x in sorted(zip(-IG,words))], sorted(-IG)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "words,IG = get_words_IG(X_test_lemm_stop, lemm_stop_test_target, count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1998',\n",
       " 'language',\n",
       " 'university',\n",
       " 'linguistic',\n",
       " 'papers',\n",
       " 'conference',\n",
       " 'remove',\n",
       " 'click',\n",
       " 'free',\n",
       " 'research']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_tf_10 = CountVectorizer(vocabulary=words[:10])\n",
    "X_train_tf_10 = count_vect_tf_10.fit_transform(lemm_stop_train_data)\n",
    "X_test_tf_10 = count_vect_tf_10.transform(lemm_stop_test_data)\n",
    "count_vect_tf_10.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_tf_100 = CountVectorizer(vocabulary=words[:100])\n",
    "X_train_tf_100 = count_vect_tf_100.fit_transform(lemm_stop_train_data)\n",
    "X_test_tf_100 = count_vect_tf_100.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_tf_1000 = CountVectorizer(vocabulary=words[:1000])\n",
    "X_train_tf_1000 = count_vect_tf_1000.fit_transform(lemm_stop_train_data)\n",
    "X_test_tf_1000 = count_vect_tf_1000.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1998',\n",
       " 'language',\n",
       " 'university',\n",
       " 'linguistic',\n",
       " 'papers',\n",
       " 'conference',\n",
       " 'remove',\n",
       " 'click',\n",
       " 'free',\n",
       " 'research']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_bin_10 = CountVectorizer(vocabulary=words[:10], binary=True)\n",
    "X_train_bin_10 = count_vect_bin_10.fit_transform(lemm_stop_train_data)\n",
    "X_test_bin_10 = count_vect_bin_10.transform(lemm_stop_test_data)\n",
    "count_vect_bin_10.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_bin_100 = CountVectorizer(vocabulary=words[:100], binary=True)\n",
    "X_train_bin_100 = count_vect_bin_100.fit_transform(lemm_stop_train_data)\n",
    "X_test_bin_100 = count_vect_bin_100.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_bin_1000 = CountVectorizer(vocabulary=words[:1000], binary=True)\n",
    "X_train_bin_1000 = count_vect_bin_1000.fit_transform(lemm_stop_train_data)\n",
    "X_test_bin_1000 = count_vect_bin_1000.transform(lemm_stop_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bern 10 acc: 0.9862542955326461\tprecision: 0.9411764705882353\trecall: 0.9795918367346939\n",
      "bern 100 acc: 0.9965635738831615\tprecision: 1.0\r",
      "ecall: 0.9795918367346939\n",
      "bern 1000 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "bern = naive_bayes.BernoulliNB()\n",
    "bern.fit(X_train_bin_10, lemm_stop_train_target)\n",
    "bern_y_hat = bern.predict(X_test_bin_10)\n",
    "bern_acc = bern.score(X_test_bin_10, lemm_stop_test_target)\n",
    "bern_prec = sklearn.metrics.precision_score(lemm_stop_test_target, bern_y_hat)\n",
    "bern_rec = sklearn.metrics.recall_score(lemm_stop_test_target, bern_y_hat)\n",
    "print(\"bern 10 acc: {}\\tprecision: {}\\trecall: {}\".format(bern_acc, bern_prec, bern_rec))\n",
    "\n",
    "bern.fit(X_train_100, lemm_stop_train_target)\n",
    "bern_y_hat = bern.predict(X_test_bin_100)\n",
    "bern_acc = bern.score(X_test_bin_100, lemm_stop_test_target)\n",
    "bern_prec = sklearn.metrics.precision_score(lemm_stop_test_target, bern_y_hat)\n",
    "bern_rec = sklearn.metrics.recall_score(lemm_stop_test_target, bern_y_hat)\n",
    "print(\"bern 100 acc: {}\\tprecision: {}\\recall: {}\".format(bern_acc, bern_prec, bern_rec))\n",
    "\n",
    "bern.fit(X_train_bin_1000, lemm_stop_train_target)\n",
    "bern_y_hat = bern.predict(X_test_bin_1000)\n",
    "bern_acc = bern.score(X_test_bin_1000, lemm_stop_test_target)\n",
    "bern_prec = sklearn.metrics.precision_score(lemm_stop_test_target, bern_y_hat)\n",
    "bern_rec = sklearn.metrics.recall_score(lemm_stop_test_target, bern_y_hat)\n",
    "print(\"bern 1000 acc: {}\\tprecision: {}\\trecall: {}\".format(bern_acc, bern_prec, bern_rec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial binary  10 acc: 0.9484536082474226\tprecision: 0.9473684210526315\trecall: 0.7346938775510204\n",
      "multinomial binary  100 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n",
      "multinomial binary  1000 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "nm_bin = naive_bayes.MultinomialNB()\n",
    "nm_bin.fit(X_train_bin_10, lemm_stop_train_target)\n",
    "nm_bin_y_hat = nm_bin.predict(X_test_bin_10)\n",
    "nm_bin_acc = nm_bin.score(X_test_bin_10, lemm_stop_test_target)\n",
    "nm_bin_prec = sklearn.metrics.precision_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "nm_bin_rec = sklearn.metrics.recall_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "print(\"multinomial binary  10 acc: {}\\tprecision: {}\\trecall: {}\".format(nm_bin_acc, nm_bin_prec, nm_bin_rec))\n",
    "\n",
    "nm_bin = naive_bayes.MultinomialNB()\n",
    "nm_bin.fit(X_train_bin_100, lemm_stop_train_target)\n",
    "nm_bin_y_hat = nm_bin.predict(X_test_bin_100)\n",
    "nm_bin_acc = nm_bin.score(X_test_bin_100, lemm_stop_test_target)\n",
    "nm_bin_prec = sklearn.metrics.precision_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "nm_bin_rec = sklearn.metrics.recall_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "print(\"multinomial binary  100 acc: {}\\tprecision: {}\\trecall: {}\".format(nm_bin_acc, nm_bin_prec, nm_bin_rec))\n",
    "\n",
    "\n",
    "nm_bin = naive_bayes.MultinomialNB()\n",
    "nm_bin.fit(X_train_bin_1000, lemm_stop_train_target)\n",
    "nm_bin_y_hat = nm_bin.predict(X_test_bin_1000)\n",
    "nm_bin_acc = nm_bin.score(X_test_bin_1000, lemm_stop_test_target)\n",
    "nm_bin_prec = sklearn.metrics.precision_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "nm_bin_rec = sklearn.metrics.recall_score(lemm_stop_test_target, nm_bin_y_hat)\n",
    "print(\"multinomial binary  1000 acc: {}\\tprecision: {}\\trecall: {}\".format(nm_bin_acc, nm_bin_prec, nm_bin_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial TF  10 acc: 0.9484536082474226\tprecision: 0.9473684210526315\trecall: 0.7346938775510204\n",
      "multinomial TF  100 acc: 0.9965635738831615\tprecision: 1.0\trecall: 0.9795918367346939\n",
      "multinomial TF  1000 acc: 0.9896907216494846\tprecision: 1.0\trecall: 0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "mn_tf = naive_bayes.MultinomialNB()\n",
    "mn_tf.fit(X_train_tf_10, lemm_stop_train_target)\n",
    "mn_tf_y_hat = mn_tf.predict(X_test_tf_10)\n",
    "mn_tf_acc = mn_tf.score(X_test_tf_10, lemm_stop_test_target)\n",
    "mn_tf_prec = sklearn.metrics.precision_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "mn_tf_rec = sklearn.metrics.recall_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "print(\"multinomial TF  10 acc: {}\\tprecision: {}\\trecall: {}\".format(mn_tf_acc, mn_tf_prec, mn_tf_rec))\n",
    "\n",
    "mn_tf = naive_bayes.MultinomialNB()\n",
    "mn_tf.fit(X_train_tf_100, lemm_stop_train_target)\n",
    "mn_tf_y_hat = mn_tf.predict(X_test_tf_100)\n",
    "mn_tf_acc = mn_tf.score(X_test_tf_100, lemm_stop_test_target)\n",
    "mn_tf_prec = sklearn.metrics.precision_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "mn_tf_rec = sklearn.metrics.recall_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "print(\"multinomial TF  100 acc: {}\\tprecision: {}\\trecall: {}\".format(mn_tf_acc, mn_tf_prec, mn_tf_rec))\n",
    "\n",
    "mn_tf = naive_bayes.MultinomialNB()\n",
    "mn_tf.fit(X_train_tf_1000, lemm_stop_train_target)\n",
    "mn_tf_y_hat = mn_tf.predict(X_test_tf_1000)\n",
    "mn_tf_acc = mn_tf.score(X_test_tf_1000, lemm_stop_test_target)\n",
    "mn_tf_prec = sklearn.metrics.precision_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "mn_tf_rec = sklearn.metrics.recall_score(lemm_stop_test_target, mn_tf_y_hat)\n",
    "print(\"multinomial TF  1000 acc: {}\\tprecision: {}\\trecall: {}\".format(mn_tf_acc, mn_tf_prec, mn_tf_rec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "SV = svm.SVC()\n",
    "#print(type(X_train_svm), np.array(IG[:50]).shape)\n",
    "#X_train_svm = np.vstack((X_train_svm, np.array(IG[:50])))\n",
    "#print(X_train_svm.shape)\n",
    "#X_test_svm = np.append(X_train_svm,np.array(IG[:50])[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,1000,10):\n",
    "    cv = CountVectorizer(vocabulary=words[:i], binary=True)\n",
    "    X_train_svm = cv.fit_transform(lemm_stop_train_data)\n",
    "    X_test_svm = cv.transform(lemm_stop_test_data)\n",
    "    SV.fit(X_train_svm, lemm_stop_train_data)\n",
    "    print(i, \" : \", SV.score(X_test_svm, lemm_stop_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
